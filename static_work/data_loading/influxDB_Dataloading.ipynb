{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfluxDB data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing orginal and preprocessed data into influxDB bu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from influxdb_client import InfluxDBClient, Point\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark setup\n",
    "spark = SparkSession.builder.appName(\"mema\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function reads data from a CSV file into a Spark DataFrame and writes the data to an InfluxDB database. It iterates through the Spark DataFrame, creating InfluxDB Points for each row and excluding the \"timestamp\" column. The function includes error handling to print any encountered exceptions during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for writing to spark\n",
    "def process_spark_dataframe(spark, csv_file_path, influxdb_bucket):\n",
    "    # Read CSV file into a Spark DataFrame\n",
    "    spark_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # InfluxDB setup\n",
    "    influxdb_url = \"http://localhost:8086\"\n",
    "    influxdb_token = \"BD8-Z9Rcrb-lCOQcWJ7h-5kzuvX0ZIWMmlw8uza-1psB_8jjQOSbXC8XyOaWyjEBUWUZTXZtp-rYhnQPEZShxw==\"\n",
    "    influxdb_org = \"mema_org\"\n",
    "\n",
    "    with InfluxDBClient(url=influxdb_url, token=influxdb_token, org=influxdb_org) as client:\n",
    "        write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "        # Iterate over rows in the Spark DataFrame\n",
    "        for row in spark_df.collect():\n",
    "            try:\n",
    "                # Create InfluxDB Point\n",
    "                point = Point(\"HAI_measurements\")\n",
    "\n",
    "                # Add fields to the InfluxDB Point, excluding \"timestamp\"\n",
    "                for col_name in spark_df.columns:\n",
    "                    if col_name != \"timestamp\":\n",
    "                        col_value = row[col_name]\n",
    "                        point.field(col_name, col_value)\n",
    "\n",
    "                # Write the InfluxDB Point to the database\n",
    "                write_api.write(influxdb_bucket, influxdb_org, point)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the csv files path for the loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CSV file path \n",
    "csv_test1 = \"/Users/emmatosato/Documents/UNI_Locale/Erasmus/OST/ost-sm-change-detection/data_analysis/merged_data/test_pd1.csv\"\n",
    "csv_test2 = \"/Users/emmatosato/Documents/UNI_Locale/Erasmus/OST/ost-sm-change-detection/data_analysis/merged_data/test_pd2.csv\"\n",
    "csv_complete = \"/Users/emmatosato/Documents/UNI_Locale/Erasmus/OST/ost-sm-change-detection/data_analysis/merged_data/complete_pd.csv\"\n",
    "\n",
    "csv_test1_proc = \"/Users/emmatosato/Documents/UNI_Locale/Erasmus/OST/ost-sm-change-detection/data_analysis/preprocessed_data/scaled_test1.csv\"\n",
    "csv_test2_proc = \"/Users/emmatosato/Documents/UNI_Locale/Erasmus/OST/ost-sm-change-detection/data_analysis/preprocessed_data/scaled_test2.csv\"\n",
    "csv_complete_proc = \"/Users/emmatosato/Documents/UNI_Locale/Erasmus/OST/ost-sm-change-detection/data_analysis/preprocessed_data/scaled_complete.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function for scaled and not scaled testing set, and also for the train1 and train 2 sets, merged together in the data analysis step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_spark_dataframe(spark, csv_test1, \"Test1\")\n",
    "process_spark_dataframe(spark, csv_test2, \"Test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_spark_dataframe(spark, csv_test1_proc, \"Test1Processed\")\n",
    "process_spark_dataframe(spark, csv_test2_proc, \"Test2Processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_spark_dataframe(spark, csv_complete, \"CompleteTrain\")\n",
    "process_spark_dataframe(spark, csv_complete_proc, \"CompleteTrainProcessed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
