{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "import json\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import influxdb_client\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"En1iX5zqnyR_AT71S6Ahz8_Hs78nrJHwEkZDDksf4J6reHJNqXzbaMEXbmBjy7I-bdzp2k8fy7E1FjU1f2ZWsA==\"\n",
    "org = \"mema_org\"\n",
    "bucket = \"mema_bucket\"\n",
    "url = \"http://localhost:8086\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions of functions\n",
    "def calculate_delay_of_detection(true_change_indexes, detected_indexes):\n",
    "    \n",
    "    delays = [index - detected_index for detected_index, index in zip(detected_indexes, true_change_indexes)]\n",
    "\n",
    "    if len(delays) > 0:\n",
    "        average_delay = sum(delays) / len(delays)\n",
    "        return average_delay\n",
    "    else:\n",
    "        return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_false_detection_rate(true_change_indexes, detected_indexes):\n",
    "    total_drifts = len(true_change_indexes)\n",
    "    total_detected = len(detected_indexes)\n",
    "    false_detections = total_detected - total_drifts\n",
    "    fdr = false_detections / total_drifts\n",
    "    return fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_miss_detection_rate(true_change_indexes, detected_indexes):\n",
    "    total_drifts = len(true_change_indexes)\n",
    "    total_detected = len(detected_indexes)\n",
    "    mdr = (total_drifts - total_detected) / total_drifts\n",
    "    return mdr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_rate_of_drift(detected_indexes, total_time):\n",
    "    total_detected = len(detected_indexes)\n",
    "    rod = total_detected / total_time\n",
    "    return rod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page-Hinkley Test Implementation\n",
    "class PageHinkley:\n",
    "    def __init__(self, min_instances=30, delta=0.005, threshold=50, alpha=1-0.0001):\n",
    "        self.min_instances = min_instances\n",
    "        self.delta = delta\n",
    "        self.threshold = threshold\n",
    "        self.alpha = alpha\n",
    "        self.cum_sum = 0\n",
    "        self.mean = 0\n",
    "        self.n = 0\n",
    "\n",
    "    def add_element(self, value):\n",
    "        if self.n < self.min_instances:\n",
    "            self.n += 1\n",
    "            self.mean = self.mean + (value - self.mean) / self.n\n",
    "            return False\n",
    "\n",
    "        self.cum_sum = max(0, self.alpha * self.cum_sum + (value - self.mean - self.delta))\n",
    "\n",
    "        self.mean = self.mean + (value - self.mean) / self.n\n",
    "        self.n += 1\n",
    "\n",
    "        if self.cum_sum > self.threshold:\n",
    "            self.cum_sum = 0\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect change using Page-Hinkley\n",
    "def detect_change(data, columns_to_monitor):\n",
    "    results = {}\n",
    "    for column in columns_to_monitor:\n",
    "        ph = PageHinkley()\n",
    "        results[column] = []\n",
    "        for i, value in enumerate(data[column]):\n",
    "            if ph.add_element(value):\n",
    "                results[column].append(i)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the model from the file\n",
    "model = joblib.load('../albert/model/random_forest_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaiConsumer:\n",
    "    def __init__(self, topic, bootstrap_servers):\n",
    "        self.topic = topic\n",
    "        self.bootstrap_servers = bootstrap_servers\n",
    "        self.consumer = KafkaConsumer(\n",
    "            self.topic,\n",
    "            bootstrap_servers=self.bootstrap_servers,\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=True,\n",
    "            value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "    def consume(self):\n",
    "        \n",
    "        columns_to_scale_and_monitor = ['P1_FCV01D', 'P1_PIT01', 'P1_FT01', 'P2_VIBTR01', 'x1001_05_SETPOINT_OUT']\n",
    "        counter = 0\n",
    "        # write to file\n",
    "        with influxdb_client.InfluxDBClient(url=url, token=token, org=org) as client:\n",
    "            write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "            for i, message in enumerate(self.consumer):\n",
    "                message = message.value\n",
    "                # selected_columns = {key: value for key, value in message.items() if key in columns_to_scale_and_monitor}\n",
    "                df = pd.DataFrame([message])\n",
    "                df = df.iloc[:,1:]\n",
    "                random_forest_detection = model.predict(df)\n",
    "                stream_size = random_forest_detection.shape[0]\n",
    "                change_points = detect_change(df, columns_to_scale_and_monitor)\n",
    "\n",
    "                # Evaluate change detection performance using EDDM\n",
    "                # average_delay_eddm = calculate_delay_of_detection(true_attack_indexes, change_points)\n",
    "                fdr_ph = calculate_false_detection_rate(random_forest_detection, change_points)\n",
    "                mdr_ph = calculate_miss_detection_rate(random_forest_detection, change_points)\n",
    "                rod_ph = calculate_rate_of_drift(change_points, total_time=stream_size - 500)\n",
    "\n",
    "                p = influxdb_client.Point(\"Change_Detection_PageHinkley\").field('fdr_ph', fdr_ph)\n",
    "                write_api.write(bucket, org, p)\n",
    "\n",
    "                p = influxdb_client.Point(\"Change_Detection_PageHinkley\").field('mdr_ph', mdr_ph)\n",
    "                write_api.write(bucket, org, p)\n",
    "                \n",
    "                p = influxdb_client.Point(\"Change_Detection_PageHinkley\").field('rod_ph', rod_ph)\n",
    "                write_api.write(bucket, org, p)\n",
    "                \n",
    "                sleep(3)\n",
    "                if i > 100:\n",
    "                    break\n",
    "\n",
    "            client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'hai-input'\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "consumer = HaiConsumer(topic, bootstrap_servers)\n",
    "consumer.consume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    file_path = '../../data_loading/hai-23_05/hai-test1.csv'\n",
    "    test1 = pd.read_csv(file_path)\n",
    "    label_file_path = '../../data_loading/hai-23_05/label-test1.csv'\n",
    "    label1 = pd.read_csv(label_file_path)\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(test1.iloc[:,1:], label1['label'], test_size=0.2, random_state=42)\n",
    "    print(test_data.dtypes)\n",
    "    print(type(test_data))\n",
    "    print(test_data.shape)\n",
    "    stream_size = test_data.shape[0]\n",
    "    print(test1.dtypes)\n",
    "    print(label1.dtypes)\n",
    "    # Accessing the indexes of true attack instances\n",
    "    true_attack_indexes = test_labels[test_labels == 1].index\n",
    "    print(true_attack_indexes)\n",
    "\n",
    "    # Select columns to preprocess and monitor\n",
    "    columns_to_scale_and_monitor = ['P1_FCV01D', 'P1_PIT01', 'P1_FT01', 'P2_VIBTR01', 'x1001_05_SETPOINT_OUT']\n",
    "\n",
    "    # preprocessed_data = preprocess_data(data, columns_to_scale_and_monitor)\n",
    "    change_points = detect_change(test_data, columns_to_scale_and_monitor)\n",
    "\n",
    "    # Evaluate change detection performance using EDDM\n",
    "    # average_delay_eddm = calculate_delay_of_detection(true_attack_indexes, change_points)\n",
    "    fdr_ph = calculate_false_detection_rate(true_attack_indexes, change_points)\n",
    "    mdr_ph = calculate_miss_detection_rate(true_attack_indexes, change_points)\n",
    "    rod_ph = calculate_rate_of_drift(change_points, total_time=stream_size - 500)\n",
    "\n",
    "    print(fdr_ph)\n",
    "    print(mdr_ph)\n",
    "    print(rod_ph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1_FCV01D                float64\n",
      "P1_FCV01Z                float64\n",
      "P1_FCV02D                float64\n",
      "P1_FCV02Z                float64\n",
      "P1_FCV03D                float64\n",
      "                          ...   \n",
      "x1002_07_SETPOINT_OUT    float64\n",
      "x1002_08_SETPOINT_OUT    float64\n",
      "x1003_10_SETPOINT_OUT    float64\n",
      "x1003_18_SETPOINT_OUT    float64\n",
      "x1003_24_SUM_OUT         float64\n",
      "Length: 86, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(10800, 86)\n",
      "timestamp                 object\n",
      "P1_FCV01D                float64\n",
      "P1_FCV01Z                float64\n",
      "P1_FCV02D                float64\n",
      "P1_FCV02Z                float64\n",
      "                          ...   \n",
      "x1002_07_SETPOINT_OUT    float64\n",
      "x1002_08_SETPOINT_OUT    float64\n",
      "x1003_10_SETPOINT_OUT    float64\n",
      "x1003_18_SETPOINT_OUT    float64\n",
      "x1003_24_SUM_OUT         float64\n",
      "Length: 87, dtype: object\n",
      "timestamp    object\n",
      "label         int64\n",
      "dtype: object\n",
      "Index([49597, 20286, 27368, 24985,  1583, 12146, 20237, 24516, 49546,  1632,\n",
      "       ...\n",
      "       30421, 24758, 24923, 49831, 20304, 27348, 24505,  5815, 17132, 49418],\n",
      "      dtype='int64', length=635)\n",
      "-0.9921259842519685\n",
      "0.9921259842519685\n",
      "0.0004854368932038835\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdr_eddm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfdr_eddm\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(mdr_eddm)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(rod_eddm)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fdr_eddm' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
