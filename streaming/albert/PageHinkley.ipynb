{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page-Hinkley Test Implementation\n",
    "class PageHinkley:\n",
    "    def __init__(self, min_instances=30, delta=0.005, threshold=50, alpha=1-0.0001):\n",
    "        self.min_instances = min_instances\n",
    "        self.delta = delta\n",
    "        self.threshold = threshold\n",
    "        self.alpha = alpha\n",
    "        self.cum_sum = 0\n",
    "        self.mean = 0\n",
    "        self.n = 0\n",
    "\n",
    "    def add_element(self, value):\n",
    "        if self.n < self.min_instances:\n",
    "            self.n += 1\n",
    "            self.mean = self.mean + (value - self.mean) / self.n\n",
    "            return False\n",
    "\n",
    "        self.cum_sum = max(0, self.alpha * self.cum_sum + (value - self.mean - self.delta))\n",
    "\n",
    "        self.mean = self.mean + (value - self.mean) / self.n\n",
    "        self.n += 1\n",
    "\n",
    "        if self.cum_sum > self.threshold:\n",
    "            self.cum_sum = 0\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess dataset\n",
    "def preprocess_data(data, columns_to_scale):\n",
    "    data = data.fillna(method='ffill')  # Fill missing values\n",
    "    scaler = StandardScaler()\n",
    "    data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect change using Page-Hinkley\n",
    "def detect_change(data, columns_to_monitor):\n",
    "    results = {}\n",
    "    for column in columns_to_monitor:\n",
    "        ph = PageHinkley()\n",
    "        results[column] = []\n",
    "        for i, value in enumerate(data[column]):\n",
    "            if ph.add_element(value):\n",
    "                results[column].append(i)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import influxdb_client\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"En1iX5zqnyR_AT71S6Ahz8_Hs78nrJHwEkZDDksf4J6reHJNqXzbaMEXbmBjy7I-bdzp2k8fy7E1FjU1f2ZWsA==\"\n",
    "org = \"mema_org\"\n",
    "bucket = \"mema_bucket\"\n",
    "url = \"http://localhost:8086\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with influxdb_client.InfluxDBClient(url=url, token=token, org=org) as client:\n",
    "    write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "    for message in consumer:\n",
    "        message = message.value\n",
    "        if len(message) < 10:\n",
    "            break\n",
    "        try:\n",
    "            point = json.dumps(message)\n",
    "            print(point)\n",
    "            print(type(point))\n",
    "            point = eval(point)\n",
    "            point_values = []\n",
    "            for key in point:\n",
    "                print(key)\n",
    "                print(point[key])\n",
    "                p = influxdb_client.Point(\"point\").field(key, point[key])\n",
    "                write_api.write(bucket, org, p)\n",
    "                point_values.append(point[key])\n",
    "        except NotImplementedError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gjirafa\\AppData\\Local\\Temp\\ipykernel_15644\\530487815.py:7: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = data.fillna(method='ffill')  # Fill missing values\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    file_path = '../../data_loading/hai-23_05/hai-train1.csv' \n",
    "    data = load_dataset(file_path)\n",
    "\n",
    "    # Select columns to preprocess and monitor\n",
    "    columns_to_scale_and_monitor = ['P1_FCV01D', 'P1_PIT01', 'P1_FT01', 'P2_VIBTR01', 'x1001_05_SETPOINT_OUT']\n",
    "\n",
    "    preprocessed_data = preprocess_data(data, columns_to_scale_and_monitor)\n",
    "    change_points = detect_change(preprocessed_data, columns_to_scale_and_monitor)\n",
    "\n",
    "    # Display the results\n",
    "    with influxdb_client.InfluxDBClient(url=url, token=token, org=org) as client:\n",
    "        write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "        for i in range(0, 10):\n",
    "            p = influxdb_client.Point(\"Albert_Mao_Test\").field(f'Col_{i}', (i+5))\n",
    "            write_api.write(bucket, org, p)\n",
    "        # for column, points in change_points.items():\n",
    "            \n",
    "        #     if points:\n",
    "        #         p = influxdb_client.Point(\"PageHinkley\").field(str(column), points)\n",
    "        #         write_api.write(bucket, org, p)\n",
    "        #         print(f\"Changes detected in {column} at indices: {points}\")\n",
    "        #     else:\n",
    "        #         print(f\"No significant changes detected in {column}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
