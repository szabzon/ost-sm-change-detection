{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "import json\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import influxdb_client\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"LuDJ6F7gMj8KqZyms0Q3krl0HKsHPYNvPO6k6dEl2lvzSSYyxyVy951S2hg-loMep5v71fZidBVWb2MqH1qdkA==\"\n",
    "org = \"mema_org\"\n",
    "bucket = \"mema_bucket\"\n",
    "url = \"http://localhost:8086\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADWIN:\n",
    "    def __init__(self, delta=0.002):\n",
    "        self.delta = delta\n",
    "        self.window = []\n",
    "        self.total = 0\n",
    "        self.variance = 0\n",
    "        self.width = 0\n",
    "\n",
    "    def add_element(self, value):\n",
    "        cutpoint_detected = False\n",
    "        self.window.append(value)\n",
    "        self.width += 1\n",
    "        self.total += value\n",
    "        self.update_variance(value)\n",
    "\n",
    "        if self.width > 1:\n",
    "            cutpoint = self.find_cut()\n",
    "            if cutpoint:\n",
    "                self.window = self.window[cutpoint:]\n",
    "                self.width = len(self.window)\n",
    "                self.total = sum(self.window)\n",
    "                self.recalculate_variance()\n",
    "                cutpoint_detected = True\n",
    "        return cutpoint_detected\n",
    "\n",
    "    def update_variance(self, value):\n",
    "        mean = self.total / self.width\n",
    "        self.variance += (value - mean) * (value - self.total / (self.width - 1))\n",
    "\n",
    "    def find_cut(self):\n",
    "        for i in range(1, self.width):\n",
    "            w0 = i\n",
    "            w1 = self.width - i\n",
    "            mean0 = sum(self.window[:i]) / w0\n",
    "            mean1 = sum(self.window[i:]) / w1\n",
    "            var0 = sum((x - mean0) ** 2 for x in self.window[:i]) / w0\n",
    "            var1 = sum((x - mean1) ** 2 for x in self.window[i:]) / w1\n",
    "            m = 1 / w0 + 1 / w1\n",
    "            epsilon = ((2 / (m - 1)) * log(4 / self.delta)) ** 0.5\n",
    "            if abs(mean0 - mean1) > epsilon:\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    def recalculate_variance(self):\n",
    "        mean = self.total / self.width\n",
    "        self.variance = sum((x - mean) ** 2 for x in self.window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the model from the file\n",
    "model = joblib.load('../albert/model/random_forest_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df):\n",
    "    label = df.pop('attack')\n",
    "    return df, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaiConsumer:\n",
    "    def __init__(self, topic, bootstrap_servers):\n",
    "        self.topic = topic\n",
    "        self.bootstrap_servers = bootstrap_servers\n",
    "        self.consumer = KafkaConsumer(\n",
    "            self.topic,\n",
    "            bootstrap_servers=self.bootstrap_servers,\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=True,\n",
    "            value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "    def consume(self):\n",
    "        # Initialize ADWIN detector\n",
    "        adwin = ADWIN(delta=0.002)\n",
    "\n",
    "        counter = 0\n",
    "        # Perform change detection on the test set\n",
    "        global change_points\n",
    "        global accuracies\n",
    "        global whole_df\n",
    "        global whole_labels\n",
    "        change_points = []\n",
    "        accuracies = []\n",
    "        whole_df = pd.DataFrame()\n",
    "        whole_labels = pd.Series()\n",
    "        # write to file\n",
    "        with influxdb_client.InfluxDBClient(url=url, token=token, org=org) as client:\n",
    "            write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "            for i, message in enumerate(self.consumer):\n",
    "                \n",
    "                message = message.value\n",
    "                # selected_columns = {key: value for key, value in message.items() if key in columns_to_scale_and_monitor}\n",
    "                df = pd.DataFrame([message])\n",
    "                x_i, y_i = split_df(df)\n",
    "                x_i = x_i.iloc[0:1]\n",
    "                \n",
    "                whole_df =pd.concat([whole_df,x_i], ignore_index=True)\n",
    "                whole_labels = pd.concat([whole_labels,y_i], ignore_index=True)\n",
    "\n",
    "                # Predict using the RandomForest model\n",
    "                pred = model.predict(x_i)\n",
    "\n",
    "                # Check for a change point using ADWIN\n",
    "                if adwin.add_element(pred == int(y_i)):\n",
    "                    change_points.append(i)\n",
    "\n",
    "                # Calculate accuracy at each step\n",
    "                accuracy = model.score(whole_df.iloc[:i + 1], whole_labels.iloc[:i + 1])\n",
    "                accuracies.append(accuracy)\n",
    "                \n",
    "                if accuracy >= 0.5:\n",
    "                    accuracy = 1\n",
    "                else:\n",
    "                    accuracy = 0\n",
    "                p = influxdb_client.Point(\"HAI_ChangeDetection_ADWIN\").field('current_state', int(accuracy))\n",
    "                write_api.write(bucket, org, p)\n",
    "\n",
    "                \n",
    "                p = influxdb_client.Point(\"HAI_ChangeDetection_ADWIN\").field('change_detected', int(y_i))\n",
    "                write_api.write(bucket, org, p)\n",
    "                \n",
    "            client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'hai-preprocessed-mao-3'\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "consumer = HaiConsumer(topic, bootstrap_servers)\n",
    "consumer.consume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
